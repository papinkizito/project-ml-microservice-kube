


(.devops) voclabs:~/environment/project-ml-microservice-kubernetes (main) $ ./run_kubernetes.sh 
Error from server (AlreadyExists): pods "latest" already exists
NAME     READY   STATUS              RESTARTS   AGE
latest   0/1     ContainerCreating   0          13s
error: unable to forward port because pod is not running. Current status=Pending
(.devops) voclabs:~/environment/project-ml-microservice-kubernetes (main) $ ./run_kubernetes.sh 
Error from server (AlreadyExists): pods "latest" already exists
NAME     READY   STATUS    RESTARTS   AGE
latest   1/1     Running   0          34s
Forwarding from 127.0.0.1:8000 -> 80
Forwarding from [::1]:8000 -> 80

## This comes when making predictions -- kubernetes window ---####
Handling connection for 8000
Handling connection for 8000
Handling connection for 8000

voclabs:~/environment $ source ~/.devops/bin/activate
(.devops) voclabs:~/environment $ cd project-ml-microservice-kubernetes/
(.devops) voclabs:~/environment/project-ml-microservice-kubernetes (main) $ ./make_prediction.sh 
Port: 8000
{
  "prediction": [
    20.35373177134412
  ]
}
(.devops) voclabs:~/environment/project-ml-microservice-kubernetes (main) $ ./make_prediction.sh 
Port: 8000
{
  "prediction": [
    20.35373177134412
  ]
}
(.devops) voclabs:~/environment/project-ml-microservice-kubernetes (main) $ ./make_prediction.sh 
Port: 8000
{
  "prediction": [
    20.35373177134412
  ]
}
(.devops) voclabs:~/environment/project-ml-microservice-kubernetes (main) $ 



####### Docker Windows #####


Successfully built 1a2429a10889
Successfully tagged project-microservices:udacity
REPOSITORY                    TAG             IMAGE ID       CREATED          SIZE
project-microservices         udacity         1a2429a10889   22 minutes ago   1.19GB
gcr.io/k8s-minikube/kicbase   v0.0.39         67a4b1138d2d   2 months ago     1.05GB
python                        3.7.3-stretch   34a518642c76   3 years ago      929MB
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 980-651-477
[2023-06-08 13:21:58,424] INFO in app: JSON payload: 
{'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}
[2023-06-08 13:21:58,445] INFO in app: Inference payload DataFrame: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-06-08 13:21:58,454] INFO in app: Scaling Payload: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-06-08 13:21:58,460] INFO in app: Ourput prediction: [20.35373177134412]
172.17.0.1 - - [08/Jun/2023 13:21:58] "POST /predict HTTP/1.1" 200 -
[2023-06-08 13:22:00,647] INFO in app: JSON payload: 
{'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}
[2023-06-08 13:22:00,659] INFO in app: Inference payload DataFrame: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-06-08 13:22:00,667] INFO in app: Scaling Payload: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-06-08 13:22:00,671] INFO in app: Ourput prediction: [20.35373177134412]
172.17.0.1 - - [08/Jun/2023 13:22:00] "POST /predict HTTP/1.1" 200 -